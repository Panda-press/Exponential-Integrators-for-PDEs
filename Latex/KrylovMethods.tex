\section{Krylov Methods}
When using the above exponential integrators, we need to be able to compute the matrix exponential $e^{A}$ or more precisely the action of the exponential of a matrix to a vector $e^{A}v$
For a lower dimension, computing $e^{A}v$ is computationally cheap.
However, when the dimension of the matrix $A$ is large then computing $e^{A}v$ is computationally intensive.
One solution to this is to use Krylov subspace methods.
These algorithms take a matrix $A\in \mathbb{R}^{n\times n}$, a vector $v \in \mathbb{R}^n$ and an integer $m$ that determines the number of dimensions of the Krylov subspace used.
From these algorithms we will get a matrix $H \in \mathbb{R}^{m\times m}$ and another matrix $V \in \mathbb{R}^{n\times m}$ such that $A \approx VHV^T$ and $V^TV = I$.
$V$ contains basis vectors of the Krylov subspace and $H$ is such that $V^TAV = H$.
Where the matrix $V$ consists of basis vectors of the Krylov subspace $span(v, Av, ..., A^{m-1}v)$.
From here we get $Av \approx VHV^Tv = VH||v||e_1$.\\
We can apply this to $e^A$ as follows:
\begin{align*}
e^A &= \sum^{\infty}_{i=0}\frac{A^i}{i!}\\
&= \sum^{\infty}_{i=0}\frac{(VHV^T)^i}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^T}{i!} \\
&\text {and then when computing $e^Av$ we get}\\
e^Av &= (\sum^{\infty}_{i=0}\frac{VH^iV^T}{i!})v \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^Tv}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^i||v||e_1}{i!} \\
&= V(\sum^{\infty}_{i=0}\frac{H^i}{i!})||v||e_1 \\
&= Ve^H||v||e_1
\end{align*}
We will still need to comptue that exponential of the matrix $H$ but for $m\ll n$ this will be computationally cheap.

\subsection{Algorithms}
We now state the two Krylov subspace methods that we will be investigating, the Arnoldi and Lanczos algorithms.
Following this we will investigating the benefits and drawbacks of each algorithm.

\subsubsection{Arnoldi}
Below we present the Arnoldi algorithm.
\begin{algorithm}[H]
\caption{Arnoldi \cite{Fan2018}} %find better citation
\begin{algorithmic}
\Procedure{Arnoldi}{$A, \hat v_1,m$}
\State $v_0 \gets 0$
\For{$j = 1,2,...,m$}	
\For{$i = 1,2,...,j$}
\State$h_{ij} \gets v_i^T A v_i$
\EndFor
\State$\theta_j \gets Av_j - \sum^j_{i=1} h_{ij}v_i$
\State$h_{j+1,j} \gets ||\theta_j||$
\State$v_{j+1} \gets \theta_j/h_{j+1,j}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by $v_1,...,v_m$ and $H$ is give by $h_1,...,h_m$.\\

\subsubsection{Lanczos}
The algorithm below is the Lanczos algorithm and it requires a symmetric matrix. \cite{Moler2003}
\begin{algorithm}[H]
\caption{Lanczos \cite{OJALVO1970}}
\begin{algorithmic}
\Procedure{Lanczos}{$A$ symetric$, \hat v_1,m$}
\State $v_0 = 0$
\For{$i = 1,2,...,m$}	
\State$\beta_i \gets || \hat v_i ||$
\State$v_i \gets \hat v_i / || \hat v_i ||$
\State$\alpha_i \gets v_i^T A v_i$
\State$\hat v_{i+1} \gets Av_i - \alpha_iv_i - \beta_iv_{i-1}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by ${v_1,...,v_m}$ and $H$ is tridiagonal with the leading diagonal being $\alpha_1, ..., \alpha_m$ and the upper and lower diagonals being $\beta_2,...,\beta_m$.
\subsubsection{Benefits and Drawbacks}
The main difference of the above algorithm is that the Lanczos algorithm requires $A$ to be a symetric matrix, whereas the Arnoldi algorithm does not have this requirement.
While this does allow the Arnoldi algorithm to have more broad applications than the Lanczos method, it does have a drawback as the Lanczos algorithm is faster than the Arnoldi algorithm as a result of the internal loop.
As a result it is neccessary to carefully select which method to use based on the problem that we are working on.

\subsection{Numerical Analysis}
Here we present a results from Y.SAAD\cite{Saad1992}. Which shows the convergence rates for the Arnoldi method.

\begin{theorem}
    Let A be any square matrix and let \(\rho=||A||_2\) then the error of the approximation of \(e^{\tau A}v\) is such that
    \[ ||e^{\tau A}v - \beta V_m e^{H_m}e_1||_2 \leq 2\beta \frac{(\tau \rho)^m e^{\tau \rho}}{m!} \] where \[\beta = ||v||_2\]
\end{theorem}
From here we see that the approximation gets more accurate for smaller values of $\tau$, as well as for larger values of $m$ given that $\tau \rho\leq1$.

\subsection{Numerical Accuracy and Performance}
We now investigate how these Krylov methods compare to already existing methods for computing the matrix exponential such as those used in SciPy under 

\verb|scipy.sparse.linalg.expm_multiply|\cite{AlMohy2011}\cite{Higham2010}.
We will compare timings as well as the required depth of the Krylov subsapce neccessary for accurate computations.
The matrix $A \in \mathbb{R}^{n \times n}$ being used is sparse and tridiagonal with $2\tau$ along the leading diagonal and $-1\tau$ along the upper and lower diagonal for some $\tau > 0$.
We use SciPy to compute a reference solution and then compare this to the approximation produced by the Krylov methods.
The error is the euclidian distance between the reference solution and the approximate solution.
We will compare for a variety of matrix sizes and values of $\tau$

Below, we compare the error to the dimension of the the Krylov subspace $m$.
% \begin{figure}[H]
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=8192 Tau=0.01.png} % Change filename to your image
%         \caption{$m$ vs error with matrix size $N=8192$ and $\tau = 0.01$}
%         \label{fig:mEKrylov1}
%     \end{minipage}\hfill
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=8192 Tau=1.0.png} % Change filename to your image
%         \caption{$m$ vs error with matrix size $N=8192$ and $\tau = 1$}
%         \label{fig:mEKrylov2}
%     \end{minipage}\hfill
% \end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=32768 Tau=0.01.png} % Change filename to your image
        \caption{$m$ vs error with matrix size $N=32768$ and $\tau = 0.01$}
        \label{fig:mEKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=32768 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs error with matrix size $N=32768$ and $\tau = 1$}
        \label{fig:mEKrylov2}
    \end{minipage}\hfill
\end{figure}
We see the error decreases for larger Krylov subspaces before leveling out, demonstraighting convergence with the SciPy reference solution.
For $\tau = 0.01$ we see that a smaller Krylov subsapce of around $6$ is needed in order to achieve this convergence than for $\tau = 1$ which needs about $16$.
This is expected as shown in the theorem above.

Here we observe the relation between the computation time and the dimension of the Krylov subsapce $m$.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v Comp Time Results for N=8192 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs computation time with matrix size 8192}
        \label{fig:mCTKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v Comp Time Results for N=32768 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs computation time with matrix size 32768}
        \label{fig:mCTKrylov2}
    \end{minipage}\hfill
\end{figure}
We observe that as expected the larger the Krylov subspace, the longer the compute time.
We also see that the Lanczos method performs faster the the Arnoldi method, again as expected.
The difference in performance appears to be more pronounced at larger Krylov sizes, with the Arnoldi method growing faster that then Lanczos.

\subsection{Further Implemetational Aspects}
Throughout both the Arnoldi and Lanczos algorithms the computation of $DN(u_h(t))v$ for some vector $v$ is required.
One possible approach to this is to comptute $DN(u_h(t))$ from $N(u_h)$ using automatic differentiation.
This process is handled by the use of UFL\cite{Alnaes2014} and DUNE\cite{Bastian2021}.
From here computing $DN(u_h(t))v$ is straight forward.

The computation, storage and hence use of an explicitly computed $DN(u_h(t))$ can be very demanding.
As we only need $DN(u_h(t))v$ we can avoid needing to assemble $DN(u_h(t))$ using matrix free methods with the following approximations:
\begin{align*}
    Au &= DN(u)u\\
    &\approx \frac{N(u(1+\epsilon))-N(u)}{\epsilon}
\end{align*}
for some small $\epsilon$.