\section{Krylov Methods}
When using the above exponential integrators, we need to be able to compute the matrix exponential $e^{A}$ or, more precisely, the action of the exponential of a matrix to a vector $e^{A}v$.
For a lower dimension, computing $e^{A}v$ is computationally cheap.
However, when the dimension of the matrix $A$ is large, then computing $e^{A}v$ is computationally intensive.
One solution to this is to use Krylov subspace methods.
These algorithms take a matrix $A\in \mathbb{R}^{n\times n}$, a vector $v \in \mathbb{R}^n$ and an integer $m$ that determines the number of dimensions of the Krylov subspace used.
From these algorithms, we will get a matrix $H \in \mathbb{R}^{m\times m}$ and another matrix $V \in \mathbb{R}^{n\times m}$ such that $A \approx VHV^T$ and $V^TV = I$.
$V$ contains basis vectors of the Krylov subspace $span(v, Av, ..., A^{m-1}v)$ and $H$ is such that $V^TAV = H$.
From here we get $Av \approx VHV^Tv = VH||v||e_1$.\\
We can apply this to $e^A$ as follows:
\begin{align*}
e^A &= \sum^{\infty}_{i=0}\frac{A^i}{i!}\\
&= \sum^{\infty}_{i=0}\frac{(VHV^T)^i}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^T}{i!} \\
&\text {and then when computing $e^Av$ we get}\\
e^Av &= (\sum^{\infty}_{i=0}\frac{VH^iV^T}{i!})v \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^Tv}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^i||v||e_1}{i!} \\
&= V(\sum^{\infty}_{i=0}\frac{H^i}{i!})||v||e_1 \\
&= Ve^H||v||e_1
\end{align*}
We will still need to compute the exponential of the matrix $H$ but for $m\ll n$ this will be computationally cheap.

When computing $\varphi_k(A)$, notice that the following holds:
\begin{align*}
    \varphi_k(A)v &= V\varphi_k(H)||v||e_1\\
\end{align*}
where $V,H$ are generated by one of the Krylov Methods above. 
We demonstrate this below:
\begin{align*}
    \varphi_k(A)v &= \int^1_0e^{(1-\theta)A}\frac{\theta^{k-1}}{(k-1)!}d\theta v\\
    &= \int^1_0e^{(1-\theta) A}d\theta v\\
    &= \int^1_0e^{(1-\theta) A}vd\theta\\
\intertext{applying a Krylov method giving $VH||v||e_1 \approx Av$ we get}
    &= \int^1_0Ve^{(1-\theta) H}||v||e_1d\theta\\
    &= V\int^1_0e^{(1-\theta) H}d||v||e_1\\
    &= V\varphi_k(H)||v||e_1
\end{align*}
As a result of this, we will only need to genarate a single subspace for the numerical integrations.

It should also be noted, for the second order exponential integrator, that the Krylov space used to compute $(\varphi_1(A))R(u_h(t))$ is the same subspace used to $\tau\varphi_1(c_2 A)R(u_h(t))$.
This is also true for the computation of $e^{A}u_h(t)$ and $e^{c_2A}u_h(t)$.

\subsection{Algorithms}
We now state the two Krylov subspace methods that we will be investigating, the Arnoldi and Lanzcos algorithms.
Following this, we will investigate the benefits and drawbacks of each algorithm.

\subsubsection{Arnoldi}
Below we present the Arnoldi algorithm.
\begin{algorithm}[H]
\caption{Arnoldi \cite{Higham2008}} %find better citation
\begin{algorithmic}
\Procedure{Arnoldi}{$A, \hat v_1,m$}
\State $v_0 \gets 0$
\For{$j = 1,2,...,m$}	
\For{$i = 1,2,...,j$}
\State$h_{ij} \gets v_i^T A v_j$
\EndFor
\State$\theta_j \gets Av_j - \sum^j_{i=1} h_{ij}v_i$
\State$h_{j+1,j} \gets ||\theta_j||$
\State$v_{j+1} \gets \theta_j/h_{j+1,j}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by $v_1,...,v_m$ and $H$ is give by $h_1,...,h_m$.\\

\subsubsection{Lanczos}
The algorithm below is the Lanzcos algorithm and it requires a symmetric matrix. \cite{Moler2003}
\begin{algorithm}[H]
\caption{Lanzcos\cite{OJALVO1970}}
\begin{algorithmic}
\Procedure{Lanczos }{$A$ symmetric$, \hat v_1,m$}
\State $v_0 = 0$
\For{$i = 1,2,...,m$}	
\State$\beta_i \gets || \hat v_i ||$
\State$v_i \gets \hat v_i / || \hat v_i ||$
\State$\alpha_i \gets v_i^T A v_i$
\State$\hat v_{i+1} \gets Av_i - \alpha_iv_i - \beta_iv_{i-1}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by ${v_1,...,v_m}$ and $H$ is tridiagonal with the leading diagonal being $\alpha_1, ..., \alpha_m$ and the upper and lower diagonals being $\beta_2,...,\beta_m$.

It should be noted that most of the computational cost arrises from applying $A$ to a vector.
For both algorithms this is only computed once per step, meaning that in total $m$ matrix vector products are required.
\subsubsection{Benefits and Drawbacks}
The main difference between the above algorithms is that the Lanczos algorithm requires $A$ to be a symmetric matrix, whereas the Arnoldi algorithm does not have this requirement.
This does allow the Arnoldi algorithm to have more broad applications than the Lanczos method.
However, the Lanczos algorithm is faster than the Arnoldi algorithm as a result of the internal loop in the Arnoldi algorithm.
As a result, it is necessary to carefully select which method to use.
This will depend upon the problem that we are working on.

\subsection{Numerical Analysis}
Here we present results from Saad\cite{Saad1992}. This shows the convergence rates for the Arnoldi method.

\begin{theorem}
    Let A be any square matrix and let \(\rho=||A||_2\) then the error of the approximation of \(e^{\tau A}v\) is such that
    \[ ||e^{\tau A}v - \beta V_m e^{H_m}e_1||_2 \leq 2\beta \frac{(\tau \rho)^m e^{\tau \rho}}{m!} \] where \[\beta = ||v||_2\]
\end{theorem}
From here, we see that the approximation gets more accurate for smaller values of $\tau$, as well as for larger values of $m$ given that $\tau \rho\leq1$.

\subsection{Numerical Accuracy and Performance}
We now investigate how these Krylov methods compare to existing methods for computing the action of the matrix exponential to a vector such as those used in SciPy under 

\verb|scipy.sparse.linalg.expm_multiply|\cite{AlMohy2011}\cite{Higham2010}.

We will compare timings, as well as the required depth of the Krylov subspace necessary for accurate computations.
The matrix $A \in \mathbb{R}^{n \times n}$ being used is sparse and tridiagonal with $2\tau$ along the leading diagonal and $-\tau$ along the upper and lower diagonal for some $\tau > 0$.
We use this matrix as matrices of this sort arrise for second order spacial derivatives, such as with the finite difference method.
We use SciPy to compute a reference solution and then compare this to the approximation produced by the Krylov methods.
The error is the Euclidian distance between the reference solution and the approximate solution.
We will compare for a variety of matrix sizes and values of $\tau$

Below, we compare the error to the dimension of the the Krylov subspace $m$.
% \begin{figure}[H]
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=8192 Tau=0.01.png} % Change filename to your image
%         \caption{$m$ vs error with matrix size $N=8192$ and $\tau = 0.01$}
%         \label{fig:mEKrylov1}
%     \end{minipage}\hfill
%     \centering
%     \begin{minipage}{0.49\textwidth}
%         \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=8192 Tau=1.0.png} % Change filename to your image
%         \caption{$m$ vs error with matrix size $N=8192$ and $\tau = 1$}
%         \label{fig:mEKrylov2}
%     \end{minipage}\hfill
% \end{figure}
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=32768 Tau=0.01.png} % Change filename to your image
        \caption{$m$ vs error with matrix size $n=32768$ and $\tau = 0.01$}
        \label{fig:mEKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v E Results for N=32768 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs error with matrix size $n=32768$ and $\tau = 1$}
        \label{fig:mEKrylov2}
    \end{minipage}\hfill
\end{figure}
We see the error decreases for larger Krylov subspaces before levelling out, demonstrating convergence with the SciPy reference solution.
For $\tau = 0.01$, we see that a smaller Krylov subspace of around $6$ is needed in order to achieve this convergence whereas for $\tau = 1$ a subspace of depth about $16$ is sufficient.
This is expected as shown in the theorem above.

Here, we observe the relationship between the computation time and the dimension of the Krylov subspace $m$.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v Comp Time Results for N=8192 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs computation time with matrix size 8192}
        \label{fig:mCTKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{Graphs/KrylovMethods/M v Comp Time Results for N=32768 Tau=1.0.png} % Change filename to your image
        \caption{$m$ vs computation time with matrix size 32768}
        \label{fig:mCTKrylov2}
    \end{minipage}\hfill
\end{figure}
With $n=32768$ and a Krylov size of 10 the Lanczos and Arnoldi methods required just over $10^{-2}$ and $2\times 10^{-2}$ seconds respectively,
whereas for a Krylov size of 50 the computation time was $8\times 10^{-2}$ and $3\times 10^{-1}$ respectively.
As expected, the larger the Krylov subspace, the longer the time to compute.
We also see that the Lanczos method performs faster the the Arnoldi method, again, as expected.
The difference in performance appears to be more pronounced at larger Krylov sizes, with the Arnoldi method growing faster than the Lanczos method.
This is due to the extra computations from the additional internal loop of the Arnoldi method.
It should also be noted that the matrix being used is sparse, as it is tridiagonal, and as a result, extra compute time from the internal loop may not be as pronounced as for matrices that are dense.

\subsection{Further Implementational Aspects}
Throughout both the Arnoldi and Lanzcos algorithms the computation of $DN(u_h(t))v$ for some vector $v$ is required.
One possible approach is to comptute $DN(u_h(t))$ from $N(u_h)$ using automatic differentiation.
This process is handled by the use of UFL\cite{Alnaes2014} and DUNE\cite{Bastian2021}.
From here, computing $DN(u_h(t))v$ is straightforward.

The computation, storage and hence use of an explicitly computed $DN(u_h(t))$ can be very demanding.
As we only need $DN(u_h(t))v$ we can avoid needing to assemble $DN(u_h(t))$ using matrix free methods with the following approximations:
\begin{align*}
    Au &= DN(u)u\\
    &\approx \frac{N(u(1+\epsilon))-N(u)}{\epsilon}
\end{align*}
for some small $\epsilon$.