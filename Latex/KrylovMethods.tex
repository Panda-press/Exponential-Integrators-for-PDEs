\section{Krylov Methods}
When using the above exponential integrators, we need to be able to compute the matrix exponential $e^{A}$ or more precicely the action of the exponential of a matrix to a vector $e^{A}v$
For a lower dimention, computing $e^{A}v$ is computationally cheap.
However, when the dimention of the matrix $A$ is large then computing $e^{A}v$ is computationally intensive.
One solution to this is to use Krylov subspace methods.
These algorithms take a matrix $A\in \mathbb{R}^{n\times n}$, a vector $v \in \mathbb{R}^n$ and an integer $m$ that determines the number of dimensions of the Krylov subspace used.
From these algorithms we will get a matrix $H \in \mathbb{R}^{m\times m}$ and another matrix $V \in \mathbb{R}^{n\times m}$ such that $A \approx VHV^T$ and $VV^T = I$.
Where the matrix $V$ consists of basis vectors of the Krylov subspace $span(v, Av, ..., A^{m-1}v)$.
From here we get $Av \approx VHV^Tv = VH||v||e_1$.\\
We can apply this to $e^A$ as follows:
\begin{align*}
e^A &= \sum^{\infty}_{i=0}\frac{A^i}{i!}\\
&= \sum^{\infty}_{i=0}\frac{(VHV^T)^i}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^T}{i!} \\
&\text {and then when computing $e^Av$ we get}\\
e^Av &= (\sum^{\infty}_{i=0}\frac{VH^iV^T}{i!})v \\
&= \sum^{\infty}_{i=0}\frac{VH^iV^Tv}{i!} \\
&= \sum^{\infty}_{i=0}\frac{VH^i||v||e_1}{i!} \\
&= V(\sum^{\infty}_{i=0}\frac{H^i}{i!})||v||e_1 \\
&= Ve^H||v||e_1
\end{align*}

\subsection{Algorithms}
We now state the two Krylov subspace methods that we will be investigating, the Arnoldi and Lanczos algorithms.
Following this we will investigating the benifits and drawbacks of each algorithm.

\subsubsection{Arnoldi}
Below we present the Arnoldi algorithm.
\begin{algorithm}[H]
\caption{Arnoldi \cite{Fan2018}} %find better citation
\begin{algorithmic}
\Procedure{Arnoldi}{$A, \hat v_1,m$}
\State $v_0 \gets 0$
\For{$j = 1,2,...,m$}	
\For{$i = 1,2,...,j$}
\State$h_{ij} \gets v_i^T A v_i$
\EndFor
\State$\theta_j \gets Av_j - \sum^j_{i=1} h_{ij}v_i$
\State$h_{j+1,j} \gets ||\theta_j||$
\State$v_{j+1} \gets \theta_j/h_{j+1,j}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by $v_1,...,v_m$ and $H$ is give by $h_1,...,h_m$.\\

\subsubsection{Lanczos}
The algorithm below is the Lanczos algorithm and it requires a symmetric matrix. \cite{Moler2003}
\begin{algorithm}[H]
\caption{Lanczos \cite{OJALVO1970}}
\begin{algorithmic}
\Procedure{Lanczos}{$A$ symetric$, \hat v_1,m$}
\State $v_0 = 0$
\For{$i = 1,2,...,m$}	
\State$\beta_i \gets || \hat v_i ||$
\State$v_i \gets \hat v_i / || \hat v_i ||$
\State$\alpha_i \gets v_i^T A v_i$
\State$\hat v_{i+1} \gets Av_i - \alpha_iv_i - \beta_iv_{i-1}$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
Here $V$ is given by ${v_1,...,v_m}$ and $H$ is tridiagonal with the leading diagonal being $\alpha_1, ..., \alpha_m$ and the upper and lower diagonals being $\beta_2,...,\beta_m$.
\subsubsection{Benifits and Drawbacks}
The main difference of the above algorithm is that the Lanczos algorithm requires $A$ to be a symetric matrix, whereas the Arnoldi algorithm does not have this requirement.
While this does allow the Arnoldi algorithm to have more broad applications then the Lanczos method, it does have a drawback as the Lanczos algorithm is faster than the Arnoldi algorithm.
As a result it is neccessary to carefully select which method to use based on the problem that we are working on.

\subsection{Numerical Analysis}
Here we present a results from Y.SAAD\cite{Saad1992}. Which shows the convergesnce rates for the Arnoldi method.

\begin{theorem}
    Let A be any square matrix and let \(\rho=||A||_2\) then the error of the approximation of \(e^{\tau A}v\) is such that
    \[ ||e^{\tau A}v - \beta V_m e^{H_m}e_1||_2 \leq 2\beta \frac{(\tau \rho)^m e^{\tau \rho}}{m!} \]
\end{theorem}
From here we see that the approximation gets more accurate for smaller values of $\tau$, as well as for larger values of $m$ given that $\tau \rho\leq1$.

\subsection{Numerical Accuracy and Performance}
We now investigate how these Krylov methods compare to already existing methods for computing the matrix exponential such as those used in SciPy under \verb|scipy.sparse.linalg.expm_multiply|\cite{AlMohy2011}\cite{Higham2010}.
We will compare timings as well as the required depth of the Krylov subsapce neccessary for accurate computations.
The matrix $A \in mathbb{R}^{n \times n}$ being used is sparse and tridiagonal with $2\tau$ along the leading diagonal and $-1\tau$ along the upper and lower diagonal.
We use SciPy to compute a reference solution and then compare this to the approximation produced by the Krylov methods.
The error is the euclidian distance between the reference solution and the approximate solution.
We will compare for a variety of matrix sizes and values of $\tau$

Below, we compare the error to the dimention of the the Krylov subspace $m$.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{$m$ vs error with matrix size TBD}
        \label{fig:mEKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{$m$ vs error with matrix size TBD}
        \label{fig:mEKrylov2}
    \end{minipage}\hfill
\end{figure}
We observe that 

Here we observe the relation between the computation time and the dimention of the Krylov subsapce $m$.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{$m$ vs computation time with matrix size TBD}
        \label{fig:mCTKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{$m$ vs computation time with matrix size TBD}
        \label{fig:mCTKrylov2}
    \end{minipage}\hfill
\end{figure}
We observe that 

Now we measure the time required to get below a given error.
\begin{figure}[H]
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{Time to get below an error of}
        \label{fig:ETKrylov1}
    \end{minipage}\hfill
    \centering
    \begin{minipage}{0.49\textwidth}
        \includegraphics[width=1\textwidth]{example-image-a} % Change filename to your image
        \caption{Time to get below an error of}
        \label{fig:ETKrylov2}
    \end{minipage}\hfill
\end{figure}
We observe that 

\subsection{Automatic Differentiation}
Throughout both the Arnoldi and Lanczos algorithms the computation of $DN(u_h(t))v$ form some vector $v$ is required.
One possible approach to this is to comptute $DN(u_h(t))$ from $N(u_h)$ using automatic differentiation.
This process is handled by the use of UFL\cite{Alnaes2014} and DUNE\cite{Bastian2021}.
From here computing $DN(u_h(t))v$ is straight forward.

\subsection{Matrix Free Methods}
The computation, storage and hence use of an explicitly computed $DN(u_h(t))$ can be very demanding.
As we only need $DN(u_h(t))v$ we can avoid needing to assemble $DN(u_h(t))$ using the following approximations:
\begin{align*}
    Au &= DN(u)\\
    &\approx \frac{N(u+\epsilon)-N(u)}{\epsilon}
\end{align*}
for some small $\epsilon$